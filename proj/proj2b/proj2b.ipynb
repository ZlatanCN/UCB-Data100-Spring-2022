{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"proj2b.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2B: Spam/Ham Classification - Build Your Own Model\n",
    "\n",
    "## Feature Engineering, Classification, Cross Validation\n",
    "## Due Date: Sunday 4/28, 11:59 PM PDT\n",
    "\n",
    "**Collaboration Policy**\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about\n",
    "the project, we ask that you **write your solutions individually**. If you do\n",
    "discuss the assignments with others please **include their names** at the top\n",
    "of your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators**: *list collaborators here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "proj2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## This Assignment\n",
    "In this project, you will be building and improving on the concepts and functions that you implemented in Project 2A to create your own classifier to distinguish spam emails from ham (non-spam) emails. We will evaluate your work based on your model's accuracy and your written responses in this notebook.\n",
    "\n",
    "After this assignment, you should feel comfortable with the following:\n",
    "\n",
    "- Using `sklearn` libraries to process data and fit models\n",
    "- Validating the performance of your model and minimizing overfitting\n",
    "- Generating and analyzing precision-recall curves\n",
    "\n",
    "## Warning\n",
    "This is a **real world** dataset– the emails you are trying to classify are actual spam and legitimate emails. As a result, some of the spam emails may be in poor taste or be considered inappropriate. We think the benefit of working with realistic data outweighs these innapropriate emails, and wanted to give a warning at the beginning of the project so that you are made aware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to suppress all FutureWarnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Breakdown\n",
    "Question | Points\n",
    "--- | ---\n",
    "1 | 6\n",
    "2a | 4\n",
    "2b | 2\n",
    "3 | 3\n",
    "4 | 15\n",
    "Total | 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "p1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Setup and Recap\n",
    "\n",
    "Here we will provide a summary of Project 2A to remind you of how we cleaned the data, explored it, and implemented methods that are going to be useful for building your own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:17:41.341673Z",
     "start_time": "2019-04-03T20:17:41.330307Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style = \"whitegrid\", \n",
    "        color_codes = True,\n",
    "        font_scale = 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loading",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Loading and Cleaning Data\n",
    "\n",
    "Remember that in email classification, our goal is to classify emails as spam or not spam (referred to as \"ham\") using features generated from the text in the email. \n",
    "\n",
    "The dataset consists of email messages and their labels (0 for ham, 1 for spam). Your labeled training dataset contains 8348 labeled examples, and the unlabeled test set contains 1000 unlabeled examples.\n",
    "\n",
    "Run the following cell to load in the data into DataFrames.\n",
    "\n",
    "The `train` DataFrame contains labeled data that you will use to train your model. It contains four columns:\n",
    "\n",
    "1. `id`: An identifier for the training example\n",
    "1. `subject`: The subject of the email\n",
    "1. `email`: The text of the email\n",
    "1. `spam`: 1 if the email is spam, 0 if the email is ham (not spam)\n",
    "\n",
    "The `test` DataFrame contains 1000 unlabeled emails. You will predict labels for these emails and submit your predictions to the autograder for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('spam_ham_data.zip') as item:\n",
    "    item.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subject</th>\n",
       "      <th>email</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Subject: A&amp;L Daily to be auctioned in bankrupt...</td>\n",
       "      <td>url: http://boingboing.net/#85534171\\n date: n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Subject: Wired: \"Stronger ties between ISPs an...</td>\n",
       "      <td>url: http://scriptingnews.userland.com/backiss...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Subject: It's just too small                  ...</td>\n",
       "      <td>&lt;html&gt;\\n &lt;head&gt;\\n &lt;/head&gt;\\n &lt;body&gt;\\n &lt;font siz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Subject: liberal defnitions\\n</td>\n",
       "      <td>depends on how much over spending vs. how much...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Subject: RE: [ILUG] Newbie seeks advice - Suse...</td>\n",
       "      <td>hehe sorry but if you hit caps lock twice the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            subject  \\\n",
       "0   0  Subject: A&L Daily to be auctioned in bankrupt...   \n",
       "1   1  Subject: Wired: \"Stronger ties between ISPs an...   \n",
       "2   2  Subject: It's just too small                  ...   \n",
       "3   3                      Subject: liberal defnitions\\n   \n",
       "4   4  Subject: RE: [ILUG] Newbie seeks advice - Suse...   \n",
       "\n",
       "                                               email  spam  \n",
       "0  url: http://boingboing.net/#85534171\\n date: n...     0  \n",
       "1  url: http://scriptingnews.userland.com/backiss...     0  \n",
       "2  <html>\\n <head>\\n </head>\\n <body>\\n <font siz...     1  \n",
       "3  depends on how much over spending vs. how much...     0  \n",
       "4  hehe sorry but if you hit caps lock twice the ...     0  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_training_data = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Convert the emails to lower case as a first step to processing the text\n",
    "original_training_data['email'] = original_training_data['email'].str.lower()\n",
    "test['email'] = test['email'].str.lower()\n",
    "\n",
    "original_training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to explore the dataset above along with any specific spam and ham emails that interest you. Keep in mind that our data may contain missing values, which are handled in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:17:42.203231Z",
     "start_time": "2019-04-03T20:17:42.185104Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b1fb39d9b651ca1b",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before imputation:\n",
      "id         0\n",
      "subject    6\n",
      "email      0\n",
      "spam       0\n",
      "dtype: int64\n",
      "------------\n",
      "After imputation:\n",
      "id         0\n",
      "subject    0\n",
      "email      0\n",
      "spam       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fill any missing or NAN values\n",
    "print('Before imputation:')\n",
    "print(original_training_data.isnull().sum())\n",
    "original_training_data = original_training_data.fillna('')\n",
    "print('------------')\n",
    "print('After imputation:')\n",
    "print(original_training_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Validation Split\n",
    "\n",
    "Recall that the training data we downloaded is all the data we have available for both training models and **validating** the models that we train. We therefore split the training data into separate training and validation datsets. You will need this **validation data** to assess the performance of your classifier once you are finished training. \n",
    "\n",
    "As in Project 2A, we set the seed (random_state) to 42. **Do not modify this in the following questions, as our tests depend on this random seed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:17:42.317970Z",
     "start_time": "2019-04-03T20:17:42.294532Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-873194ed3e686dfb",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This creates a 90/10 train-validation split on our labeled data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(original_training_data, test_size = 0.1, random_state = 42)\n",
    "\n",
    "# We must do this in order to preserve the ordering of emails to labels for words_in_texts\n",
    "train = train.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "feat-eng",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Feature Engineering\n",
    "\n",
    "In order to train a logistic regression model, we need a numeric feature matrix $X$ and a vector of corresponding binary labels $y$. To address this, in Project 2A, we implemented the function `words_in_texts`, which creates numeric features derived from the email text and uses those features for logistic regression. \n",
    "\n",
    "For this project, we have provided you with an implemented version of `words_in_texts`. Remember that the function outputs a 2-dimensional NumPy array containing one row for each email text. The row should contain either a 0 or a 1 for each word in the list: 0 if the word doesn't appear in the text and 1 if the word does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_in_texts(words, texts):\n",
    "    '''\n",
    "    Args:\n",
    "        words (list): words to find\n",
    "        texts (Series): strings to search in\n",
    "    \n",
    "    Returns:\n",
    "        NumPy array of 0s and 1s with shape (n, p) where n is the\n",
    "        number of texts and p is the number of words.\n",
    "    '''\n",
    "    import numpy as np\n",
    "    indicator_array = 1 * np.array([texts.str.contains(word) for word in words]).T\n",
    "    return indicator_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to see how the function works on some dummy text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 1]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_texts(['hello', 'bye', 'world'], pd.Series(['hello', 'hello worldhello']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "classification",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### EDA and Basic Classification\n",
    "\n",
    "In Project 2A, we proceeded to visualize the frequency of different words for both spam and ham emails, and used `words_in_texts(words, train['email'])` to directly to train a classifier. We also provided a simple set of 5 words that might be useful as features to distinguish spam/ham emails. \n",
    "\n",
    "We then built a model using the using the [`LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier from `scikit-learn`.\n",
    "\n",
    "Run the following cell to see the performance of a simple model using these words and the `train` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:17:43.726012Z",
     "start_time": "2019-04-03T20:17:43.498088Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "q4-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0]]),\n",
       " array([0, 0, 0, 0, 0], dtype=int64))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_words = ['drug', 'bank', 'prescription', 'memo', 'private']\n",
    "\n",
    "X_train = words_in_texts(some_words, train['email'])\n",
    "Y_train = np.array(train['spam'])\n",
    "\n",
    "X_train[:5], Y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:17:44.593918Z",
     "start_time": "2019-04-03T20:17:43.783872Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "q5-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.7576201251164648\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "training_accuracy = model.score(X_train, Y_train)\n",
    "print(\"Training Accuracy: \", training_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our models, we are evaluating accuracy on the training set, which may provide a misleading accuracy measure. In Project 2A, we calculated various metrics to lead us to consider more ways of evaluating a classifier, in addition to overall accuracy. Below is a reference to those concepts.\n",
    "\n",
    "Presumably, our classifier will be used for **filtering**, i.e. preventing messages labeled `spam` from reaching someone's inbox. There are two kinds of errors we can make:\n",
    "- False positive (FP): a ham email gets flagged as spam and filtered out of the inbox.\n",
    "- False negative (FN): a spam email gets mislabeled as ham and ends up in the inbox.\n",
    "\n",
    "To be clear, we label spam emails as 1 and ham emails as 0. These definitions depend both on the true labels and the predicted labels. False positives and false negatives may be of differing importance, leading us to consider more ways of evaluating a classifier, in addition to overall accuracy:\n",
    "\n",
    "**Precision** measures the proportion $\\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$ of emails flagged as spam that are actually spam.\n",
    "\n",
    "**Recall** measures the proportion $\\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$ of spam emails that were correctly flagged as spam. \n",
    "\n",
    "**False-alarm rate** measures the proportion $\\frac{\\text{FP}}{\\text{FP} + \\text{TN}}$ of ham emails that were incorrectly flagged as spam. \n",
    "\n",
    "The two graphics below may help you understand precision and recall visually:\n",
    "\n",
    "![precision_recall](precision_recall.png)\n",
    "\n",
    "Note that a true positive (TP) is a spam email that is classified as spam, and a true negative (TN) is a ham email that is classified as ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "p2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Moving Forward - Building Your Own Model\n",
    "\n",
    "With this in mind, it is now your task to make the spam filter more accurate. In order to get full credit on the accuracy part of this assignment, you must get at least **88%** accuracy on the test set. To see your accuracy on the test set, you will use your classifier to predict every email in the `test` DataFrame and upload your predictions to Gradescope.\n",
    "\n",
    "**Gradescope limits you to four submissions per day**. You will be able to see your accuracy on the entire test set when submitting to Gradescope.\n",
    "\n",
    "Here are some ideas for improving your model:\n",
    "\n",
    "1. Finding better features based on the email text. Some example features are:\n",
    "    1. Number of characters in the subject / body\n",
    "    1. Number of words in the subject / body\n",
    "    1. Use of punctuation (e.g., how many '!'s were there?)\n",
    "    1. Number / percentage of capital letters \n",
    "    1. Whether the email is a reply to an earlier email or a forwarded email\n",
    "1. Finding better (and/or more) words to use as features. Which words are the best at distinguishing emails? This requires digging into the email text itself. \n",
    "1. Better data processing. For example, many emails contain HTML as well as text. You can consider extracting out the text from the HTML to help you find better words. Or, you can match HTML tags themselves, or even some combination of the two.\n",
    "1. Model selection. You can adjust parameters of your model (e.g. the regularization parameter) to achieve higher accuracy. Recall that you should use cross-validation to do feature and model selection properly! Otherwise, you will likely overfit to your training data.\n",
    "\n",
    "You may use whatever method you prefer in order to create features, but **you are not allowed to import any external feature extraction libraries**. In addition, **you are only allowed to train logistic regression models**. No decision trees, random forests, k-nearest-neighbors, neural nets, etc.\n",
    "\n",
    "We have not provided any code to do this, so feel free to create as many cells as you need in order to tackle this task. However, answering questions 1, 2, and 3 should help guide you.\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** *You may want to use your **validation data** to evaluate your model and get a better sense of how it will perform on the test set.* Note, however, that you may overfit to your validation set if you try to optimize your validation accuracy too much. Alternatively, you can perform cross-validation on the entire training set.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q7",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 1: Feature/Model Selection Process\n",
    "\n",
    "In this following cell, describe the process of improving your model. You should use at least 2-3 sentences each to address the follow questions:\n",
    "\n",
    "1. How did you find better features for your model?\n",
    "2. What did you try that worked or didn't work?\n",
    "3. What was surprising in your search for good features?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1\n",
    "manual: True\n",
    "points: 6\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "Optional: Build a Decision Tree model with reasonably good accuracy. What features does the decision tree use first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [while, was, playing, with, the, past, issues,...\n",
       "1    [url, http, diveintomark, org, archives, html,...\n",
       "2    [please, post, link, fox, original, message, f...\n",
       "3    [this, article, from, nytimes, com, has, been,...\n",
       "4    [html, head, title, tech, update, today, title...\n",
       "Name: email, dtype: object"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_with_only_words = train['email'].str.lower()\n",
    "email_with_only_words = email_with_only_words.str.findall(r'\\b(?![\\t\\n])[a-z]+-[a-z]+\\b|\\b(?![\\t\\n])[a-z]+[a-z]+[a-z]+\\b')\n",
    "email_with_only_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content-type': 1011,\n",
       " 'text': 2103,\n",
       " 'plain': 336,\n",
       " 'charset': 955,\n",
       " 'us-ascii': 40,\n",
       " 'content-transfer': 558,\n",
       " 'encoding': 558,\n",
       " 'quoted-printable': 268,\n",
       " 'aluko': 4,\n",
       " 'martin': 18,\n",
       " 'victoriagarden': 2,\n",
       " 'city': 430,\n",
       " 'lagos-nigeria': 6,\n",
       " 'attn': 24,\n",
       " 'have': 3182,\n",
       " 'immediate': 106,\n",
       " 'business': 1706,\n",
       " 'proposal': 86,\n",
       " 'that': 4239,\n",
       " 'involves': 12,\n",
       " 'which': 680,\n",
       " 'will': 3453,\n",
       " 'like': 785,\n",
       " 'invest': 128,\n",
       " 'under': 247,\n",
       " 'your': 8399,\n",
       " 'custody': 16,\n",
       " 'please': 1915,\n",
       " 'not': 3141,\n",
       " 'hesitate': 27,\n",
       " 'send': 1174,\n",
       " 'email': 3158,\n",
       " 'discuss': 36,\n",
       " 'with': 4132,\n",
       " 'you': 13966,\n",
       " 'the': 21155,\n",
       " 'details': 189,\n",
       " 'transaction': 301,\n",
       " 'terms': 117,\n",
       " 'and': 14657,\n",
       " 'condition': 13,\n",
       " 'sharing': 28,\n",
       " 'regarding': 109,\n",
       " 'urgent': 83,\n",
       " 'response': 490,\n",
       " 'highly': 56,\n",
       " 'appreciated': 14,\n",
       " 'swiftly': 4,\n",
       " 'bring': 62,\n",
       " 'commencement': 11,\n",
       " 'hope': 84,\n",
       " 'conclude': 8,\n",
       " 'this': 7271,\n",
       " 'within': 574,\n",
       " 'working': 219,\n",
       " 'days': 470,\n",
       " 'forget': 73,\n",
       " 'contact': 549,\n",
       " 'receipt': 48,\n",
       " 'mail': 715,\n",
       " 'ensure': 52,\n",
       " 'maintain': 30,\n",
       " 'absolute': 37,\n",
       " 'confidentiality': 49,\n",
       " 'regard': 21,\n",
       " 'pending': 19,\n",
       " 'urgently': 16,\n",
       " 'await': 19,\n",
       " 'best': 697,\n",
       " 'regards': 122,\n",
       " 'application': 210,\n",
       " 'octet-stream': 21,\n",
       " 'name': 3824,\n",
       " 'lst': 4,\n",
       " 'content-disposition': 31,\n",
       " 'attachment': 27,\n",
       " 'filename': 25,\n",
       " 'free': 2772,\n",
       " 'adult': 120,\n",
       " 'lifetime': 126,\n",
       " 'membership': 236,\n",
       " 'limited': 201,\n",
       " 'time': 1238,\n",
       " 'offer': 718,\n",
       " 'instant': 86,\n",
       " 'access': 383,\n",
       " 'password': 39,\n",
       " 'login': 12,\n",
       " 'zzzz': 148,\n",
       " 'spamassassin': 55,\n",
       " 'taint': 71,\n",
       " 'org': 258,\n",
       " 'sites': 411,\n",
       " 'internet': 986,\n",
       " 'for': 8442,\n",
       " 'news': 314,\n",
       " 'over': 961,\n",
       " 'million': 702,\n",
       " 'members': 206,\n",
       " 'signed': 97,\n",
       " 'last': 246,\n",
       " 'month': 486,\n",
       " 'new': 1778,\n",
       " 'are': 3843,\n",
       " 'member': 185,\n",
       " 'yet': 86,\n",
       " 'our': 3357,\n",
       " 'faq': 25,\n",
       " 'why': 295,\n",
       " 'offering': 107,\n",
       " 'advertisers': 43,\n",
       " 'pay': 369,\n",
       " 'space': 77,\n",
       " 'don': 829,\n",
       " 'true': 238,\n",
       " 'life': 771,\n",
       " 'absolutely': 248,\n",
       " 'never': 375,\n",
       " 'cent': 22,\n",
       " 'sign': 97,\n",
       " 'all': 2564,\n",
       " 'just': 1245,\n",
       " 'one': 1586,\n",
       " 'get': 1782,\n",
       " 'them': 700,\n",
       " 'give': 334,\n",
       " 'credit': 704,\n",
       " 'card': 402,\n",
       " 'information': 1482,\n",
       " 'age': 242,\n",
       " 'verification': 22,\n",
       " 'purposes': 55,\n",
       " 'only': 1489,\n",
       " 'charged': 16,\n",
       " 'believe': 180,\n",
       " 'read': 307,\n",
       " 'their': 794,\n",
       " 'conditions': 48,\n",
       " 'how': 1169,\n",
       " 'started': 190,\n",
       " 'click': 2079,\n",
       " 'links': 155,\n",
       " 'below': 818,\n",
       " 'become': 194,\n",
       " 'adults': 16,\n",
       " 'farm': 42,\n",
       " 'http': 10600,\n",
       " 'aid': 89,\n",
       " 'girls': 60,\n",
       " 'animals': 18,\n",
       " 'getting': 148,\n",
       " 'freaky': 17,\n",
       " 'sexy': 24,\n",
       " 'celebes': 15,\n",
       " 'celebst': 7,\n",
       " 'thousands': 279,\n",
       " 'xxx': 38,\n",
       " 'doing': 131,\n",
       " 'play': 101,\n",
       " 'house': 78,\n",
       " 'porn': 46,\n",
       " 'live': 220,\n",
       " 'feeds': 9,\n",
       " 'from': 3481,\n",
       " 'web': 687,\n",
       " 'cams': 28,\n",
       " 'lesbian': 17,\n",
       " 'lace': 8,\n",
       " 'teen': 39,\n",
       " 'sex': 95,\n",
       " 'fantasies': 9,\n",
       " 'schoolgirls': 8,\n",
       " 'shows': 46,\n",
       " 'jennifer': 36,\n",
       " 'simpson': 12,\n",
       " 'miami': 31,\n",
       " 'has': 1081,\n",
       " 'entertained': 11,\n",
       " 'boyfriend': 6,\n",
       " 'two': 279,\n",
       " 'years': 477,\n",
       " 'net': 2349,\n",
       " 'joe': 15,\n",
       " 'morgan': 21,\n",
       " 'manhattan': 11,\n",
       " 'unbelievable': 26,\n",
       " 'part': 195,\n",
       " 'about': 953,\n",
       " 'they': 863,\n",
       " 'removal': 277,\n",
       " 'instructions': 293,\n",
       " 'received': 521,\n",
       " 'advertisement': 122,\n",
       " 'because': 596,\n",
       " 'opted': 43,\n",
       " 'receive': 1283,\n",
       " 'offers': 426,\n",
       " 'specials': 34,\n",
       " 'through': 482,\n",
       " 'affiliated': 32,\n",
       " 'websites': 80,\n",
       " 'wish': 619,\n",
       " 'further': 301,\n",
       " 'emails': 322,\n",
       " 'error': 189,\n",
       " 'may': 785,\n",
       " 'opt-out': 76,\n",
       " 'database': 175,\n",
       " 'here': 2119,\n",
       " 'optout': 226,\n",
       " 'index': 554,\n",
       " 'html': 3957,\n",
       " 'allow': 197,\n",
       " 'hours': 363,\n",
       " 'e-mail': 1347,\n",
       " 'sent': 548,\n",
       " 'compliance': 82,\n",
       " 'exchange': 92,\n",
       " 'promotion': 99,\n",
       " 'privacy': 116,\n",
       " 'protection': 80,\n",
       " 'act': 215,\n",
       " 'section': 96,\n",
       " 'marked': 27,\n",
       " 'valid': 79,\n",
       " 'instruction': 41,\n",
       " 'tkuejsrfkmtpfwldpnauksv': 2,\n",
       " 'table': 5813,\n",
       " 'width': 11770,\n",
       " 'border': 4447,\n",
       " 'align': 7900,\n",
       " 'center': 6081,\n",
       " 'bordercolor': 753,\n",
       " 'ffccff': 15,\n",
       " 'bgcolor': 3436,\n",
       " 'font': 40479,\n",
       " 'face': 11588,\n",
       " 'verdana': 4066,\n",
       " 'href': 4623,\n",
       " 'com': 8575,\n",
       " 'simply-amateur': 4,\n",
       " 'size': 14570,\n",
       " 'color': 10825,\n",
       " 'now': 1410,\n",
       " 'simply': 361,\n",
       " 'amateur': 27,\n",
       " 'girl': 17,\n",
       " 'next': 369,\n",
       " 'door': 28,\n",
       " 'tour': 26,\n",
       " 'first': 629,\n",
       " 'photos': 9,\n",
       " 'sneeky': 2,\n",
       " 'hidden': 587,\n",
       " 'nude': 10,\n",
       " 'exibitionists': 2,\n",
       " 'cheating': 9,\n",
       " 'wives': 11,\n",
       " 'girlfriends': 5,\n",
       " 'cgi-bin': 256,\n",
       " 'list-remove': 2,\n",
       " 'removed': 803,\n",
       " 'head': 1425,\n",
       " 'body': 2215,\n",
       " 'cellpadding': 2043,\n",
       " 'cellspacing': 2072,\n",
       " 'style': 3693,\n",
       " 'border-collapse': 166,\n",
       " 'collapse': 174,\n",
       " 'arial': 7124,\n",
       " 'black': 659,\n",
       " 'club': 49,\n",
       " 'vip': 17,\n",
       " 'netnoteinc': 209,\n",
       " 'added': 64,\n",
       " 'today': 706,\n",
       " 'hot': 67,\n",
       " 'press': 67,\n",
       " 'nbsp': 10610,\n",
       " 'there': 798,\n",
       " 'were': 233,\n",
       " 'step': 288,\n",
       " 'can': 2376,\n",
       " 'account': 373,\n",
       " 'friends': 150,\n",
       " 'family': 638,\n",
       " 'yes': 274,\n",
       " 'long': 304,\n",
       " 'following': 412,\n",
       " 'pick': 29,\n",
       " 'site': 707,\n",
       " 'need': 668,\n",
       " 'sign-up': 8,\n",
       " 'asian': 34,\n",
       " 'tits': 15,\n",
       " 'patrol': 1,\n",
       " 'sinful': 2,\n",
       " 'cherries': 1,\n",
       " 'stripper': 2,\n",
       " 'sluts': 8,\n",
       " 'celebs': 4,\n",
       " 'want': 915,\n",
       " 'queen': 2,\n",
       " 'barley': 3,\n",
       " 'legal': 640,\n",
       " 'teens': 11,\n",
       " 'kinky': 1,\n",
       " 'wild': 22,\n",
       " 'babes': 6,\n",
       " 'college': 58,\n",
       " 'bad': 79,\n",
       " 'wet': 14,\n",
       " 'models': 25,\n",
       " 'boob': 3,\n",
       " 'ranch': 3,\n",
       " 'hollywood': 2,\n",
       " 'ffffff': 3039,\n",
       " 'target': 396,\n",
       " 'hardcore': 21,\n",
       " 'www': 5966,\n",
       " 'freevirginfuckers': 2,\n",
       " 'eaff': 2,\n",
       " 'map': 72,\n",
       " 'area': 179,\n",
       " 'coords': 31,\n",
       " 'shape': 75,\n",
       " 'rect': 27,\n",
       " 'img': 3529,\n",
       " 'src': 3544,\n",
       " 'art': 59,\n",
       " 'jpg': 874,\n",
       " 'usemap': 23,\n",
       " 'height': 5912,\n",
       " 'left': 1172,\n",
       " 'colspan': 1445,\n",
       " 'helvetica': 3921,\n",
       " 'valign': 1643,\n",
       " 'top': 1334,\n",
       " 'php': 388,\n",
       " 'opting-out': 1,\n",
       " 'suyjiswyhhlagomhr': 1,\n",
       " 'personal': 344,\n",
       " 'grants': 573,\n",
       " 'narrow': 31,\n",
       " 'qualify': 97,\n",
       " 'least': 239,\n",
       " 'money': 1708,\n",
       " 'guaranteed': 355,\n",
       " 'each': 683,\n",
       " 'day': 622,\n",
       " 'dollars': 453,\n",
       " 'government': 715,\n",
       " 'given': 125,\n",
       " 'away': 157,\n",
       " 'people': 1307,\n",
       " 'wide': 62,\n",
       " 'variety': 56,\n",
       " 'needs': 163,\n",
       " 'dear': 249,\n",
       " 'grant': 276,\n",
       " 'seeker': 19,\n",
       " 'blockquote': 1965,\n",
       " 'moment': 100,\n",
       " 'tell': 173,\n",
       " 'exactly': 72,\n",
       " 'amp': 447,\n",
       " 'where': 354,\n",
       " 'thinking': 75,\n",
       " 'some': 452,\n",
       " 'maybe': 67,\n",
       " 'think': 204,\n",
       " 'impossible': 34,\n",
       " 'let': 318,\n",
       " 'fact': 163,\n",
       " 'ordinary': 22,\n",
       " 'businesses': 139,\n",
       " 'across': 34,\n",
       " 'united': 290,\n",
       " 'states': 329,\n",
       " 'receiving': 305,\n",
       " 'millions': 159,\n",
       " 'these': 745,\n",
       " 'private': 205,\n",
       " 'foundation': 20,\n",
       " 'everyday': 35,\n",
       " 'who': 928,\n",
       " 'apply': 205,\n",
       " 'anyone': 357,\n",
       " 'old': 231,\n",
       " 'possible': 169,\n",
       " 'paid': 209,\n",
       " 'back': 619,\n",
       " 'ever': 329,\n",
       " 'claim': 157,\n",
       " 'slice': 24,\n",
       " 'american': 181,\n",
       " 'pie': 24,\n",
       " 'loan': 203,\n",
       " 'trying': 68,\n",
       " 'conventional': 19,\n",
       " 'bank': 247,\n",
       " 'very': 498,\n",
       " 'consuming': 13,\n",
       " 'requires': 41,\n",
       " 'lot': 151,\n",
       " 'paperwork': 31,\n",
       " 'find': 551,\n",
       " 'out': 1765,\n",
       " 'been': 804,\n",
       " 'denied': 12,\n",
       " 'agencies': 75,\n",
       " 'operate': 22,\n",
       " 'same': 238,\n",
       " 'stringent': 12,\n",
       " 'requirements': 59,\n",
       " 'banks': 37,\n",
       " 'decide': 80,\n",
       " 'much': 523,\n",
       " 'lawful': 38,\n",
       " 'amount': 170,\n",
       " 'meets': 17,\n",
       " 'criteria': 26,\n",
       " 'yours': 190,\n",
       " 'keep': 310,\n",
       " 'repaid': 10,\n",
       " 'non': 21,\n",
       " 'taxable': 10,\n",
       " 'interest': 343,\n",
       " 'none': 430,\n",
       " 'programs': 412,\n",
       " 'require': 48,\n",
       " 'check': 392,\n",
       " 'collateral': 16,\n",
       " 'security': 271,\n",
       " 'deposits': 26,\n",
       " 'co-signers': 13,\n",
       " 'even': 487,\n",
       " 'bankruptcy': 28,\n",
       " 'doesn': 84,\n",
       " 'matter': 182,\n",
       " 'tax': 139,\n",
       " 'payer': 20,\n",
       " 'citizen': 28,\n",
       " 'entitled': 41,\n",
       " 'currently': 171,\n",
       " 'federal': 211,\n",
       " 'state': 736,\n",
       " 'foundations': 18,\n",
       " 'scholarship': 12,\n",
       " 'available': 525,\n",
       " 'year': 565,\n",
       " 'billion': 119,\n",
       " 'facts': 78,\n",
       " 'every': 558,\n",
       " 'entrepreneurs': 21,\n",
       " 'start': 449,\n",
       " 'expand': 52,\n",
       " 'real': 288,\n",
       " 'estate': 143,\n",
       " 'help': 655,\n",
       " 'training': 101,\n",
       " 'better': 243,\n",
       " 'job': 159,\n",
       " 'going': 165,\n",
       " 'into': 536,\n",
       " 'themselves': 31,\n",
       " 'wanting': 28,\n",
       " 'existing': 71,\n",
       " 'should': 406,\n",
       " 'rush': 19,\n",
       " 'world': 405,\n",
       " 'largest': 52,\n",
       " 'one-stop': 11,\n",
       " 'money-shop': 10,\n",
       " 'being': 324,\n",
       " 'held': 69,\n",
       " 'div': 3569,\n",
       " 'margin-left': 184,\n",
       " 'margin-right': 170,\n",
       " 'sounds': 36,\n",
       " 'incredible': 66,\n",
       " 'living': 122,\n",
       " 'right': 1550,\n",
       " 'america': 125,\n",
       " 'wouldn': 58,\n",
       " 'know': 501,\n",
       " 'source': 98,\n",
       " 'delivers': 14,\n",
       " 'low-interest': 24,\n",
       " 'loans': 151,\n",
       " 'one-half': 10,\n",
       " 'trillion': 18,\n",
       " 'procurement': 15,\n",
       " 'contracts': 109,\n",
       " 'consulting': 50,\n",
       " 'research': 133,\n",
       " 'economy': 38,\n",
       " 'remains': 30,\n",
       " 'unpredictable': 10,\n",
       " 'greater': 32,\n",
       " 'economic': 55,\n",
       " 'development': 90,\n",
       " 'fronts': 12,\n",
       " 'more': 2042,\n",
       " 'willing': 75,\n",
       " 'than': 709,\n",
       " 'before': 397,\n",
       " 'own': 513,\n",
       " 'boss': 60,\n",
       " 'spite': 10,\n",
       " 'perception': 12,\n",
       " 'look': 227,\n",
       " 'great': 299,\n",
       " 'give-away': 10,\n",
       " 'remained': 10,\n",
       " 'incredibly': 25,\n",
       " 'huge': 148,\n",
       " 'approximately': 32,\n",
       " 'applied': 26,\n",
       " 'equal': 26,\n",
       " 'share': 160,\n",
       " 'would': 712,\n",
       " 'most': 578,\n",
       " 'somehow': 26,\n",
       " 'feel': 200,\n",
       " 'isn': 48,\n",
       " 'too': 203,\n",
       " 'red-tape': 10,\n",
       " 'however': 163,\n",
       " 'walks': 21,\n",
       " 'other': 708,\n",
       " 'benefits': 109,\n",
       " 'also': 753,\n",
       " 'buy': 342,\n",
       " 'home': 960,\n",
       " 'low': 375,\n",
       " 'income': 383,\n",
       " 'families': 15,\n",
       " 'repair': 45,\n",
       " 'rent': 12,\n",
       " 'mortgage': 354,\n",
       " 'payments': 65,\n",
       " 'utility': 18,\n",
       " 'bills': 113,\n",
       " 'purchase': 326,\n",
       " 'car': 138,\n",
       " 'groceries': 12,\n",
       " 'childcare': 10,\n",
       " 'fuel': 17,\n",
       " 'general': 168,\n",
       " 'expenses': 77,\n",
       " 'academic': 14,\n",
       " 'tutoring': 10,\n",
       " 'clothing': 16,\n",
       " 'school': 82,\n",
       " 'supplies': 62,\n",
       " 'housing': 35,\n",
       " 'assistance': 191,\n",
       " 'services': 463,\n",
       " 'summer': 43,\n",
       " 'camp': 11,\n",
       " 'debts': 31,\n",
       " 'music': 27,\n",
       " 'lessons': 34,\n",
       " 'any': 1346,\n",
       " 'extracurricular': 10,\n",
       " 'activities': 47,\n",
       " 'senior': 39,\n",
       " 'citizens': 42,\n",
       " 'taxes': 38,\n",
       " 'medical': 101,\n",
       " 'welfare': 18,\n",
       " 'someone': 143,\n",
       " 'suffered': 15,\n",
       " 'fire': 40,\n",
       " 'lose': 261,\n",
       " 'replacing': 10,\n",
       " 'necessities': 10,\n",
       " 'scholarships': 36,\n",
       " 'education': 80,\n",
       " 'preschool': 10,\n",
       " 'children': 58,\n",
       " 'nursery': 14,\n",
       " 'primary': 50,\n",
       " 'secondary': 15,\n",
       " 'schools': 14,\n",
       " 'men': 115,\n",
       " 'women': 109,\n",
       " 'athlete': 11,\n",
       " 'management': 127,\n",
       " 'engineering': 21,\n",
       " 'computer': 426,\n",
       " 'science': 46,\n",
       " 'undergraduate': 10,\n",
       " 'graduate': 14,\n",
       " 'professional': 351,\n",
       " 'foreign': 186,\n",
       " 'studies': 27,\n",
       " 'many': 534,\n",
       " 'shortest': 14,\n",
       " 'once': 280,\n",
       " 'specific': 87,\n",
       " 'results': 234,\n",
       " 'almost': 104,\n",
       " 'inevitable': 10,\n",
       " 'wants': 29,\n",
       " 'congressional': 22,\n",
       " 'mandate': 15,\n",
       " 'funds': 119,\n",
       " 'made': 509,\n",
       " 'required': 255,\n",
       " 'proper': 49,\n",
       " 'presentation': 51,\n",
       " 'request': 240,\n",
       " 'announcing': 14,\n",
       " 'margin-top': 395,\n",
       " 'complete': 395,\n",
       " 'guide': 363,\n",
       " 'everything': 186,\n",
       " 'seen': 164,\n",
       " 'heard': 65,\n",
       " 'what': 939,\n",
       " 'done': 100,\n",
       " 'put': 236,\n",
       " 'together': 47,\n",
       " 'blueprint': 10,\n",
       " 'researching': 12,\n",
       " 'locating': 22,\n",
       " 'obtaining': 41,\n",
       " 'comprehensive': 75,\n",
       " 'tool': 94,\n",
       " 'comes': 91,\n",
       " 'electronic': 71,\n",
       " 'book': 115,\n",
       " 'e-book': 79,\n",
       " 'format': 280,\n",
       " 'meaning': 41,\n",
       " 'download': 196,\n",
       " 'using': 325,\n",
       " 'minutes': 186,\n",
       " 'after': 466,\n",
       " 'order': 1149,\n",
       " 'provide': 284,\n",
       " 'sources': 64,\n",
       " 'writing': 66,\n",
       " 'procedures': 27,\n",
       " 'guidelines': 83,\n",
       " 'applying': 12,\n",
       " 'direct': 192,\n",
       " 'offered': 70,\n",
       " 'program': 793,\n",
       " 'detailed': 42,\n",
       " 'categorized': 13,\n",
       " 'listings': 43,\n",
       " 'resources': 122,\n",
       " 'phone': 779,\n",
       " 'number': 539,\n",
       " 'address': 1294,\n",
       " 'expert': 30,\n",
       " 'answer': 126,\n",
       " 'related': 69,\n",
       " 'questions': 234,\n",
       " 'charge': 152,\n",
       " 'online': 581,\n",
       " 'directory': 71,\n",
       " 'supported': 26,\n",
       " 'venture': 77,\n",
       " 'capital': 90,\n",
       " 'firms': 27,\n",
       " 'unique': 129,\n",
       " 'search': 373,\n",
       " 'generate': 97,\n",
       " 'customized': 24,\n",
       " 'listing': 120,\n",
       " 'recently': 95,\n",
       " 'announced': 21,\n",
       " 'funding': 33,\n",
       " 'small': 321,\n",
       " 'based': 271,\n",
       " 'inquiries': 19,\n",
       " 'discover': 137,\n",
       " 'sought': 19,\n",
       " 'assistant': 25,\n",
       " 'starting': 90,\n",
       " 'expanding': 23,\n",
       " 'counseling': 10,\n",
       " 'advice': 59,\n",
       " 'courtesy': 12,\n",
       " 'forms': 118,\n",
       " 'covering': 20,\n",
       " 'improvement': 197,\n",
       " 'buying': 74,\n",
       " 'homeownership': 10,\n",
       " 'land': 54,\n",
       " 'acquisition': 36,\n",
       " 'preparation': 15,\n",
       " 'health': 175,\n",
       " 'unemployed': 22,\n",
       " 'employment': 57,\n",
       " 'develop': 46,\n",
       " 'write': 178,\n",
       " 'proposals': 30,\n",
       " 'plus': 350,\n",
       " 'provides': 72,\n",
       " 'practically': 34,\n",
       " 'resident': 19,\n",
       " 'ranging': 22,\n",
       " 'already': 164,\n",
       " 'qualified': 106,\n",
       " 'hispanic': 11,\n",
       " 'christian': 51,\n",
       " 'different': 180,\n",
       " 'faiths': 10,\n",
       " 'jewish': 10,\n",
       " 'catholic': 11,\n",
       " 'having': 128,\n",
       " 'underemployed': 10,\n",
       " 'list': 1720,\n",
       " 'endless': 23,\n",
       " 'eligible': 41,\n",
       " 'use': 727,\n",
       " 'worthwhile': 19,\n",
       " 'purpose': 64,\n",
       " 'did': 214,\n",
       " 'instance': 14,\n",
       " 'could': 374,\n",
       " 'begin': 89,\n",
       " 'weight': 216,\n",
       " 'loss': 160,\n",
       " 'tuition': 17,\n",
       " 'nurse': 10,\n",
       " 'open': 150,\n",
       " 'day-care': 10,\n",
       " 'always': 186,\n",
       " 'dreamed': 18,\n",
       " 'owning': 17,\n",
       " 'then': 571,\n",
       " 'starts': 16,\n",
       " 'well': 322,\n",
       " 'another': 199,\n",
       " 'expansion': 13,\n",
       " 'possibilities': 15,\n",
       " 'must': 297,\n",
       " 'confident': 35,\n",
       " 'unhappy': 21,\n",
       " 'reason': 102,\n",
       " 'months': 308,\n",
       " 'refund': 46,\n",
       " 'entire': 123,\n",
       " 'payment': 161,\n",
       " 'asked': 65,\n",
       " 'insist': 11,\n",
       " 'entirely': 44,\n",
       " 'risk': 290,\n",
       " 'full': 266,\n",
       " 'money-back': 30,\n",
       " 'guarantee': 248,\n",
       " 'mean': 54,\n",
       " 'without': 400,\n",
       " 'feeling': 34,\n",
       " 'might': 87,\n",
       " 'taken': 107,\n",
       " 'therefore': 76,\n",
       " 'material': 40,\n",
       " 'aren': 33,\n",
       " 'completely': 92,\n",
       " 'satisfied': 41,\n",
       " 'cancel': 41,\n",
       " 'price': 539,\n",
       " 'text-align': 407,\n",
       " 'bonuses': 91,\n",
       " 'sweeten': 10,\n",
       " 'deal': 211,\n",
       " 'include': 261,\n",
       " 'four': 97,\n",
       " 'valuable': 176,\n",
       " 'gift': 84,\n",
       " 'later': 165,\n",
       " 'bonus': 260,\n",
       " 'fully': 102,\n",
       " 'featured': 31,\n",
       " 'tutorial': 13,\n",
       " 'software': 711,\n",
       " 'package': 229,\n",
       " 'info': 390,\n",
       " 'alone': 47,\n",
       " 'worth': 84,\n",
       " 'anywhere': 136,\n",
       " 'downloadable': 14,\n",
       " 'actually': 104,\n",
       " 'say': 217,\n",
       " 'accepted': 90,\n",
       " 'interactive': 47,\n",
       " 'walk': 27,\n",
       " 'grant-writing': 10,\n",
       " 'process': 138,\n",
       " 'teach': 58,\n",
       " 'competitive': 63,\n",
       " 'includes': 266,\n",
       " 'tips': 72,\n",
       " 'examples': 15,\n",
       " 'good': 342,\n",
       " 'packages': 44,\n",
       " 'glossary': 10,\n",
       " 'contacts': 67,\n",
       " 'mock': 10,\n",
       " 'grants-writing': 10,\n",
       " 'activity': 50,\n",
       " 'able': 198,\n",
       " 'compare': 112,\n",
       " 'successful': 180,\n",
       " 'insider': 93,\n",
       " 'report': 1096,\n",
       " 'ways': 84,\n",
       " 'save': 523,\n",
       " 'special': 542,\n",
       " 'contains': 202,\n",
       " 'experts': 40,\n",
       " 'techniques': 48,\n",
       " 'little': 235,\n",
       " 'known': 83,\n",
       " 'secrets': 131,\n",
       " 'tricks': 16,\n",
       " 'saving': 40,\n",
       " 'airline': 18,\n",
       " 'fares': 10,\n",
       " 'rental': 30,\n",
       " 'used': 261,\n",
       " 'auto': 145,\n",
       " 'leasing': 12,\n",
       " 'gasoline': 10,\n",
       " 'repairs': 18,\n",
       " 'insurance': 626,\n",
       " 'savings': 105,\n",
       " 'investment': 371,\n",
       " 'cards': 148,\n",
       " 'equity': 78,\n",
       " 'major': 140,\n",
       " 'appliances': 12,\n",
       " 'heating': 10,\n",
       " 'telephone': 133,\n",
       " 'food': 26,\n",
       " 'prescription': 71,\n",
       " 'drugs': 48,\n",
       " 'manual': 45,\n",
       " 'tools': 140,\n",
       " 'succeed': 53,\n",
       " 'packed': 12,\n",
       " 'guides': 34,\n",
       " 'worksheets': 10,\n",
       " 'checklists': 10,\n",
       " 'amazed': 24,\n",
       " 'simple': 385,\n",
       " 'strategies': 52,\n",
       " 'concepts': 13,\n",
       " 'easy': 434,\n",
       " 'idea': 105,\n",
       " 'hundreds': 221,\n",
       " 'sold': 107,\n",
       " 'separately': 16,\n",
       " 'taste': 35,\n",
       " 'determine': 35,\n",
       " 'feasibility': 10,\n",
       " 'fill': 405,\n",
       " 'blanks': 18,\n",
       " 'template': 33,\n",
       " 'system': 456,\n",
       " 'predict': 25,\n",
       " 'problems': 148,\n",
       " 'happen': 101,\n",
       " 'losing': 40,\n",
       " 'shirt': 15,\n",
       " 'dog': 10,\n",
       " 'ideas': 82,\n",
       " 'explanation': 14,\n",
       " 'plan': 218,\n",
       " 'make': 1154,\n",
       " 'bankers': 16,\n",
       " 'prospective': 15,\n",
       " 'partners': 120,\n",
       " 'investors': 92,\n",
       " 'line': 407,\n",
       " 'ready': 146,\n",
       " 'easily': 223,\n",
       " 'adapt': 12,\n",
       " 'exact': 43,\n",
       " 'easiest': 39,\n",
       " 'simplest': 10,\n",
       " 'products': 420,\n",
       " 'anxious': 10,\n",
       " 'invention': 10,\n",
       " 'making': 235,\n",
       " 'sure': 254,\n",
       " 'cash': 513,\n",
       " 'pocket': 77,\n",
       " 'must-know': 10,\n",
       " 'must-do': 10,\n",
       " 'ignore': 19,\n",
       " 'stand': 26,\n",
       " 'chance': 78,\n",
       " 'fail': 23,\n",
       " 'specifically': 40,\n",
       " 'designed': 80,\n",
       " 'service': 635,\n",
       " 'retail': 151,\n",
       " 'store': 68,\n",
       " 'manufacturing': 18,\n",
       " 'company': 793,\n",
       " 'nobody': 26,\n",
       " 'told': 60,\n",
       " 'raising': 14,\n",
       " 'attracting': 12,\n",
       " 'construct': 11,\n",
       " 'common': 67,\n",
       " 'mistakes': 37,\n",
       " 'traps': 15,\n",
       " 'avoid': 80,\n",
       " 'checklist': 10,\n",
       " 'entering': 35,\n",
       " 'partnership': 36,\n",
       " 'keeps': 26,\n",
       " 'costly': 18,\n",
       " 'when': 631,\n",
       " 'forming': 12,\n",
       " 'select': 269,\n",
       " 'franchise': 20,\n",
       " 'selecting': 13,\n",
       " 'step-by': 35,\n",
       " 'organized': 14,\n",
       " 'cutting': 26,\n",
       " 'costs': 94,\n",
       " 'clients': 137,\n",
       " 'mine': 31,\n",
       " 'achieved': 21,\n",
       " 'average': 107,\n",
       " 'cost': 350,\n",
       " 'reduction': 42,\n",
       " 'technique': 29,\n",
       " 'behind': 45,\n",
       " 'constructing': 10,\n",
       " 'driven': 11,\n",
       " 'marketing': 673,\n",
       " 'lead': 75,\n",
       " 'developing': 36,\n",
       " 'drive': 145,\n",
       " 'sales': 314,\n",
       " 'roof': 15,\n",
       " 'increase': 232,\n",
       " 'profits': 62,\n",
       " 'call': 652,\n",
       " 'profit': 90,\n",
       " 'planning': 83,\n",
       " 'practical': 18,\n",
       " 'sense': 31,\n",
       " 'strategy': 69,\n",
       " 'but': 840,\n",
       " 'amazingly': 12,\n",
       " 'enough': 86,\n",
       " 'understands': 12,\n",
       " 'uses': 34,\n",
       " 'success': 223,\n",
       " 'span': 2750,\n",
       " 'text-decoration': 374,\n",
       " 'fast': 200,\n",
       " 'no-frills': 10,\n",
       " 'succeeding': 12,\n",
       " 'dollar': 61,\n",
       " 'tip': 41,\n",
       " 'proven': 107,\n",
       " 'turn': 101,\n",
       " 'machine': 52,\n",
       " 'laws': 87,\n",
       " 'regulations': 29,\n",
       " 'aware': 28,\n",
       " 'errors': 22,\n",
       " 'revealed': 22,\n",
       " 'ensuring': 19,\n",
       " 'fundamentals': 15,\n",
       " 'financial': 435,\n",
       " 'imple': 11,\n",
       " 'copy': 376,\n",
       " 'enhance': 27,\n",
       " 'image': 215,\n",
       " 'customers': 191,\n",
       " 'managing': 45,\n",
       " 'solve': 31,\n",
       " 'president': 149,\n",
       " 'leading': 39,\n",
       " 'creator': 13,\n",
       " 'cd-rom': 74,\n",
       " 'author': 62,\n",
       " 'five': 146,\n",
       " 'books': 55,\n",
       " 'involved': 127,\n",
       " 'past': 162,\n",
       " 'coach': 38,\n",
       " 'manager': 106,\n",
       " 'firm': 67,\n",
       " 'seminar': 62,\n",
       " 'leader': 45,\n",
       " ...}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def words_count(train, words, column, type):\n",
    "    res = {}\n",
    "    for index, row in train.iterrows():\n",
    "        words_in_row = words[index]\n",
    "        for word in words_in_row:\n",
    "            if row[column] == type:\n",
    "                res[word] = res.get(word, 0) + 1\n",
    "    return res\n",
    "\n",
    "spam_words_count = words_count(train, email_with_only_words, 'spam', 1)\n",
    "ham_words_count = words_count(train, email_with_only_words, 'spam', 0)\n",
    "spam_words_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hasee\\AppData\\Local\\Temp\\ipykernel_20872\\716983520.py:12: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  spam_and_ham.sort_values(by='Spam + Ham', ascending=False)[spam_and_ham['Spam Ratio'] > 0.75]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Spam Count</th>\n",
       "      <th>Ham Count</th>\n",
       "      <th>Spam - Ham</th>\n",
       "      <th>Spam + Ham</th>\n",
       "      <th>Spam Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>center</td>\n",
       "      <td>6081</td>\n",
       "      <td>1976</td>\n",
       "      <td>4105</td>\n",
       "      <td>8057</td>\n",
       "      <td>0.754747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>span</td>\n",
       "      <td>2750</td>\n",
       "      <td>677</td>\n",
       "      <td>2073</td>\n",
       "      <td>3427</td>\n",
       "      <td>0.802451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140</th>\n",
       "      <td>option</td>\n",
       "      <td>2218</td>\n",
       "      <td>441</td>\n",
       "      <td>1777</td>\n",
       "      <td>2659</td>\n",
       "      <td>0.834148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2318</th>\n",
       "      <td>strong</td>\n",
       "      <td>1756</td>\n",
       "      <td>572</td>\n",
       "      <td>1184</td>\n",
       "      <td>2328</td>\n",
       "      <td>0.754296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>blockquote</td>\n",
       "      <td>1965</td>\n",
       "      <td>112</td>\n",
       "      <td>1853</td>\n",
       "      <td>2077</td>\n",
       "      <td>0.946076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9414</th>\n",
       "      <td>lion</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16015</th>\n",
       "      <td>clinic</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16640</th>\n",
       "      <td>offsetting</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9803</th>\n",
       "      <td>officejet</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5738</th>\n",
       "      <td>guage</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>956 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word  Spam Count  Ham Count  Spam - Ham  Spam + Ham  Spam Ratio\n",
       "231        center        6081       1976        4105        8057    0.754747\n",
       "961          span        2750        677        2073        3427    0.802451\n",
       "3140       option        2218        441        1777        2659    0.834148\n",
       "2318       strong        1756        572        1184        2328    0.754296\n",
       "361    blockquote        1965        112        1853        2077    0.946076\n",
       "...           ...         ...        ...         ...         ...         ...\n",
       "9414         lion           4          1           3           5    0.800000\n",
       "16015      clinic           4          1           3           5    0.800000\n",
       "16640  offsetting           4          1           3           5    0.800000\n",
       "9803    officejet           4          1           3           5    0.800000\n",
       "5738        guage           4          1           3           5    0.800000\n",
       "\n",
       "[956 rows x 6 columns]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_words_df = pd.DataFrame(list(ham_words_count.items()), columns=['Word', 'Ham Count'])\n",
    "spam_words_df = pd.DataFrame(list(spam_words_count.items()), columns=['Word', 'Spam Count'])\n",
    "spam_and_ham = pd.merge(spam_words_df, ham_words_df, left_on='Word', right_on='Word', how='outer').fillna(0)\n",
    "\n",
    "spam_and_ham['Spam Count'] = spam_and_ham['Spam Count'].astype('int')\n",
    "spam_and_ham['Ham Count'] = spam_and_ham['Ham Count'].astype('int')\n",
    "spam_and_ham['Spam - Ham'] = spam_and_ham['Spam Count'] - spam_and_ham['Ham Count']\n",
    "spam_and_ham['Spam + Ham'] = spam_and_ham['Spam Count'] + spam_and_ham['Ham Count']\n",
    "spam_and_ham['Spam Ratio'] = spam_and_ham['Spam Count'] / (spam_and_ham['Spam Count'] + spam_and_ham['Ham Count'])\n",
    "spam_and_ham = spam_and_ham[(spam_and_ham['Spam Ratio'] != 1) & (spam_and_ham['Spam Ratio'] != 0)]\n",
    "\n",
    "spam_and_ham.sort_values(by='Spam + Ham', ascending=False)[spam_and_ham['Spam Ratio'] > 0.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hasee\\AppData\\Local\\Temp\\ipykernel_20872\\479796366.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  test_words = spam_and_ham.sort_values(by='Spam + Ham', ascending=False)[spam_and_ham['Spam Ratio'] > 0.75].head(72)['Word'].tolist()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words = spam_and_ham.sort_values(by='Spam + Ham', ascending=False)[spam_and_ham['Spam Ratio'] > 0.75].head(72)['Word'].tolist()\n",
    "X_train = words_in_texts(test_words, train['email'])\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9125515772660722\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "training_accuracy = model.score(X_train, Y_train)\n",
    "print(\"Training Accuracy: \", training_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hasee\\AppData\\Local\\Temp\\ipykernel_20872\\2497040550.py:20: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  spam_and_ham_df(val).sort_values(by='Spam - Ham', ascending=False)[(spam_and_ham_df(val)['Spam Ratio'] > 0.67) & (spam_and_ham_df(val)['Spam + Ham'] >= np.mean(spam_and_ham_df(val)['Spam + Ham']))]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Spam Count</th>\n",
       "      <th>Ham Count</th>\n",
       "      <th>Spam - Ham</th>\n",
       "      <th>Spam + Ham</th>\n",
       "      <th>Spam Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>font</td>\n",
       "      <td>5383</td>\n",
       "      <td>1046</td>\n",
       "      <td>4337</td>\n",
       "      <td>6429</td>\n",
       "      <td>0.837300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>size</td>\n",
       "      <td>1804</td>\n",
       "      <td>526</td>\n",
       "      <td>1278</td>\n",
       "      <td>2330</td>\n",
       "      <td>0.774249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>color</td>\n",
       "      <td>1253</td>\n",
       "      <td>386</td>\n",
       "      <td>867</td>\n",
       "      <td>1639</td>\n",
       "      <td>0.764491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>face</td>\n",
       "      <td>1231</td>\n",
       "      <td>423</td>\n",
       "      <td>808</td>\n",
       "      <td>1654</td>\n",
       "      <td>0.744256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>align</td>\n",
       "      <td>955</td>\n",
       "      <td>226</td>\n",
       "      <td>729</td>\n",
       "      <td>1181</td>\n",
       "      <td>0.808637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>roman</td>\n",
       "      <td>43</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>60</td>\n",
       "      <td>0.716667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>account</td>\n",
       "      <td>50</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>74</td>\n",
       "      <td>0.675676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>interest</td>\n",
       "      <td>46</td>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "      <td>66</td>\n",
       "      <td>0.696970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>thank</td>\n",
       "      <td>41</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>60</td>\n",
       "      <td>0.683333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>income</td>\n",
       "      <td>39</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>56</td>\n",
       "      <td>0.696429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word  Spam Count  Ham Count  Spam - Ham  Spam + Ham  Spam Ratio\n",
       "18       font        5383       1046        4337        6429    0.837300\n",
       "188      size        1804        526        1278        2330    0.774249\n",
       "29      color        1253        386         867        1639    0.764491\n",
       "19       face        1231        423         808        1654    0.744256\n",
       "174     align         955        226         729        1181    0.808637\n",
       "..        ...         ...        ...         ...         ...         ...\n",
       "797     roman          43         17          26          60    0.716667\n",
       "951   account          50         24          26          74    0.675676\n",
       "377  interest          46         20          26          66    0.696970\n",
       "465     thank          41         19          22          60    0.683333\n",
       "308    income          39         17          22          56    0.696429\n",
       "\n",
       "[72 rows x 6 columns]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spam_and_ham_df(data_set):\n",
    "    email_with_only_words = data_set['email'].str.lower()\n",
    "    email_with_only_words = email_with_only_words.str.findall(r'\\b(?![\\t\\n])[a-z]+-[a-z]+\\b|\\b(?![\\t\\n])[a-z]+[a-z]+[a-z]+\\b')\n",
    "\n",
    "    spam_words_count = words_count(val, email_with_only_words, 'spam', 1)\n",
    "    ham_words_count = words_count(val, email_with_only_words, 'spam', 0)\n",
    "\n",
    "    ham_words_df = pd.DataFrame(list(ham_words_count.items()), columns=['Word', 'Ham Count'])\n",
    "    spam_words_df = pd.DataFrame(list(spam_words_count.items()), columns=['Word', 'Spam Count'])\n",
    "    spam_and_ham = pd.merge(spam_words_df, ham_words_df, left_on='Word', right_on='Word', how='outer').fillna(0)\n",
    "    \n",
    "    spam_and_ham['Spam Count'] = spam_and_ham['Spam Count'].astype('int')\n",
    "    spam_and_ham['Ham Count'] = spam_and_ham['Ham Count'].astype('int')\n",
    "    spam_and_ham['Spam - Ham'] = spam_and_ham['Spam Count'] - spam_and_ham['Ham Count']\n",
    "    spam_and_ham['Spam + Ham'] = spam_and_ham['Spam Count'] + spam_and_ham['Ham Count']\n",
    "    spam_and_ham['Spam Ratio'] = spam_and_ham['Spam Count'] / (spam_and_ham['Spam Count'] + spam_and_ham['Ham Count'])\n",
    "    spam_and_ham = spam_and_ham[(spam_and_ham['Spam Ratio'] != 1) & (spam_and_ham['Spam Ratio'] != 0)]\n",
    "    return spam_and_ham\n",
    "    \n",
    "spam_and_ham_df(val).sort_values(by='Spam - Ham', ascending=False)[(spam_and_ham_df(val)['Spam Ratio'] > 0.67) & (spam_and_ham_df(val)['Spam + Ham'] >= np.mean(spam_and_ham_df(val)['Spam + Ham']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.8263473053892215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hasee\\AppData\\Local\\Temp\\ipykernel_20872\\2212360612.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  test_words = spam_and_ham_df(val).sort_values(by='Spam + Ham', ascending=False)[(spam_and_ham_df(val)['Spam Ratio'] > 0.67) & (spam_and_ham_df(val)['Spam + Ham'] >= np.mean(spam_and_ham_df(val)['Spam + Ham']))]['Word'].tolist()\n"
     ]
    }
   ],
   "source": [
    "test_words = spam_and_ham_df(val).sort_values(by='Spam + Ham', ascending=False)[(spam_and_ham_df(val)['Spam Ratio'] > 0.67) & (spam_and_ham_df(val)['Spam + Ham'] >= np.mean(spam_and_ham_df(val)['Spam + Ham']))]['Word'].tolist()\n",
    "X_val = words_in_texts(test_words, val['email'])\n",
    "Y_val = np.array(val['spam'])\n",
    "\n",
    "\n",
    "training_accuracy = model.score(X_val, Y_val)\n",
    "print(\"Training Accuracy: \", training_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q8",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 2: EDA\n",
    "\n",
    "In the cell below, show a visualization that you used to select features for your model. \n",
    "\n",
    "Include:\n",
    "\n",
    "1. A plot showing something meaningful about the data that helped you during feature selection, model selection, or both.\n",
    "2. Two or three sentences describing what you plotted and its implications with respect to your features.\n",
    "\n",
    "Feel free to create as many plots as you want in your process of feature selection, but select only one for the response cell below.\n",
    "\n",
    "**You should not just produce an identical visualization to Question 3 in Project 2A.** Specifically, don't show us a bar chart of proportions, or a one-dimensional class-conditional density plot. Any other plot is acceptable, **as long as it comes with thoughtful commentary.** Here are some ideas:\n",
    "\n",
    "1. Consider the correlation between multiple features (look up correlation plots and `sns.heatmap`). \n",
    "1. Try to show redundancy in a group of features (e.g. `body` and `html` might co-occur relatively frequently, or you might be able to design a feature that captures all html tags and compare it to these). \n",
    "1. Visualize which words have high or low values for some useful statistic.\n",
    "1. Visually depict whether spam emails tend to be wordier (in some sense) than ham emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Question 2a\n",
    "\n",
    "Generate your visualization in the cell below.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "manual: True\n",
    "format: image\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T00:27:36.170465Z",
     "start_time": "2019-04-02T00:27:36.167776Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "q8-eda",
     "locked": false,
     "points": 3,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student",
     "written",
     "q_eda1"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Question 2b\n",
    "\n",
    "Write your commentary in the cell below.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2b\n",
    "manual: True\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q9",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3: ROC Curve\n",
    "\n",
    "In most cases we won't be able to get 0 false positives and 0 false negatives, so we have to compromise. For example, in the case of cancer screenings, false negatives are comparatively worse than false positives — a false negative means that a patient might not discover that they have cancer until it's too late, whereas a patient can just receive another screening for a false positive.\n",
    "\n",
    "Recall that logistic regression calculates the probability that an example belongs to a certain class. Then, to classify an example we say that an email is spam if our classifier gives it $\\ge 0.5$ probability of being spam. However, *we can adjust that cutoff*: we can say that an email is spam only if our classifier gives it $\\ge 0.7$ probability of being spam, for example. This is how we can trade off false positives and false negatives.\n",
    "\n",
    "The ROC curve shows this trade off for each possible cutoff probability. In the cell below, plot a ROC curve for your final classifier (the one you use to make predictions for Gradescope) on the training data. Refer to Lecture 20 to see how to plot an ROC curve.\n",
    "\n",
    "**Hint**: You'll want to use the `.predict_proba` method for your classifier instead of `.predict` so you get probabilities instead of binary predictions.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3\n",
    "manual: True\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q10",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Question 4: Test Predictions\n",
    "\n",
    "The following code will write your predictions on the test dataset to a CSV file. **You will need to submit this file to the \"k Test Predictions\" assignment on Gradescope to get credit for this question.**\n",
    "\n",
    "Save your predictions in a 1-dimensional array called `test_predictions`. **Please make sure you've saved your predictions to `test_predictions` as this is how part of your score for this question will be determined.**\n",
    "\n",
    "**Remember that if you've performed transformations or featurization on the training data, you must also perform the same transformations on the test data in order to make predictions.** For example, if you've created features for the words \"drug\" and \"money\" on the training data, you must also extract the same features in order to use scikit-learn's `.predict(...)` method.\n",
    "\n",
    "**Note: You may submit up to 4 times a day. If you have submitted 4 times on a day, you will need to wait until the next day for more submissions.**\n",
    "\n",
    "Note that this question is graded on an absolute scale based on the accuracy your model achieves on the overall test set, and as such, your score does not depend on your ranking on Gradescope.\n",
    "\n",
    "*The provided tests check that your predictions are in the correct format, but you must additionally submit to Gradescope to evaluate your classifier accuracy.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T00:27:38.650695Z",
     "start_time": "2019-04-02T00:27:38.469233Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "q10-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong style='color: red;'><pre style='display: inline;'>q4</pre> results:</strong></p><p><strong><pre style='display: inline;'>q4 - 1</pre> result:</strong></p><pre>    ✅ Test case passed</pre><p><strong><pre style='display: inline;'>q4 - 2</pre> result:</strong></p><pre>    ✅ Test case passed</pre><p><strong><pre style='display: inline;'>q4 - 3</pre> result:</strong></p><pre>    ❌ Test case failed\n",
       "    Trying:\n",
       "        len(test_predictions) == 1000 # must be the right number of predictions\n",
       "    Expecting:\n",
       "        True\n",
       "    **********************************************************************\n",
       "    Line 1, in q4 2\n",
       "    Failed example:\n",
       "        len(test_predictions) == 1000 # must be the right number of predictions\n",
       "    Expected:\n",
       "        True\n",
       "    Got:\n",
       "        False\n",
       "</pre>"
      ],
      "text/plain": [
       "q4 results:\n",
       "    q4 - 1 result:\n",
       "        ✅ Test case passed\n",
       "\n",
       "    q4 - 2 result:\n",
       "        ✅ Test case passed\n",
       "\n",
       "    q4 - 3 result:\n",
       "        ❌ Test case failed\n",
       "        Trying:\n",
       "            len(test_predictions) == 1000 # must be the right number of predictions\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q4 2\n",
       "        Failed example:\n",
       "            len(test_predictions) == 1000 # must be the right number of predictions\n",
       "        Expected:\n",
       "            True\n",
       "        Got:\n",
       "            False"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d15e30e2a961277d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "The following cell generates a CSV file with your predictions. **You must submit this CSV file to the \"Project 2B Test Predictions\" assignment on Gradescope to get credit for this question.**\n",
    "\n",
    "Note that the file will appear in your DataHub, you must navigate to the `hw11` directory in your DataHub to download the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T00:27:39.986326Z",
     "start_time": "2019-04-02T00:27:38.385Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8dd1bfadcbe08b00",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array length 835 does not match index length 1000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[305], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming that your predictions on the test set are stored in a 1-dimensional array called\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# test_predictions. Feel free to modify this cell as long you create a CSV in the right format.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Construct and save the submission:\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m submission_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m\"\u001b[39m: test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_predictions,\n\u001b[0;32m     10\u001b[0m }, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     11\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39misoformat(datetime\u001b[38;5;241m.\u001b[39mnow())\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     12\u001b[0m submission_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubmission_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(timestamp), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\pandas\\core\\frame.py:733\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    727\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    728\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    729\u001b[0m     )\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:448\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    444\u001b[0m missing \u001b[38;5;241m=\u001b[39m arrays\u001b[38;5;241m.\u001b[39misna()\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;66;03m# GH10856\u001b[39;00m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;66;03m# raise ValueError if only scalars in dict\u001b[39;00m\n\u001b[1;32m--> 448\u001b[0m     index \u001b[38;5;241m=\u001b[39m _extract_index(arrays[\u001b[38;5;241m~\u001b[39mmissing])\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    450\u001b[0m     index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:690\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lengths[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m    686\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    687\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlengths[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    688\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    689\u001b[0m         )\n\u001b[1;32m--> 690\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    692\u001b[0m     index \u001b[38;5;241m=\u001b[39m default_index(lengths[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: array length 835 does not match index length 1000"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Assuming that your predictions on the test set are stored in a 1-dimensional array called\n",
    "# test_predictions. Feel free to modify this cell as long you create a CSV in the right format.\n",
    "\n",
    "# Construct and save the submission:\n",
    "submission_df = pd.DataFrame({\n",
    "    \"Id\": test['id'], \n",
    "    \"Class\": test_predictions,\n",
    "}, columns=['Id', 'Class'])\n",
    "timestamp = datetime.isoformat(datetime.now()).split(\".\")[0]\n",
    "submission_df.to_csv(\"submission_{}.csv\".format(timestamp), index=False)\n",
    "\n",
    "print('Created a CSV file: {}.'.format(\"submission_{}.csv\".format(timestamp)))\n",
    "print('You may now upload this CSV file to Gradescope for scoring.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You have completed Project 2B!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q4 results:\n",
       "    q4 - 1 result:\n",
       "        ✅ Test case passed\n",
       "\n",
       "    q4 - 2 result:\n",
       "        ✅ Test case passed\n",
       "\n",
       "    q4 - 3 result:\n",
       "        ❌ Test case failed\n",
       "        Trying:\n",
       "            len(test_predictions) == 1000 # must be the right number of predictions\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q4 2\n",
       "        Failed example:\n",
       "            len(test_predictions) == 1000 # must be the right number of predictions\n",
       "        Expected:\n",
       "            True\n",
       "        Got:\n",
       "            False"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
